{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Non linear programming\n",
        "\n",
        "In this notebook we present three method to find the minimum of the function\n",
        "\n",
        "$$f(x) = x^4 - 3x^3 +2 $$\n",
        "\n",
        "This methods are: Quasi Newton Method (BFGS), gradient descent (slowest yet most used in deep learning) and Newton Raphson method (fastest method, commonly can not be used)\n",
        "\n",
        "As an excercise, we first program the three methods, then use the corresponding libraries in python"
      ],
      "metadata": {
        "id": "CsqbckYAtu8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function"
      ],
      "metadata": {
        "id": "VU6PEwZEvRib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def f(x):\n",
        "  return x**4 - 3*x**3 + 2\n",
        "\n",
        "def f_deriv(x):\n",
        "  return 3*x**3 - 9*x**2\n",
        "\n",
        "def f_seg_deriv(x):\n",
        "  return 6*x**2 - 18*x"
      ],
      "metadata": {
        "id": "Szz7hEphvQlL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quasi Newton Method"
      ],
      "metadata": {
        "id": "tQvuk-JknOtA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXYsvzEDb653",
        "outputId": "31317618-e52a-4d6c-8087-9cf6f62ff367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum found at x = 3.0000000000000386\n"
          ]
        }
      ],
      "source": [
        "def quasi_newton(f_deriv, x_0, tol=1e-7, max_iter=100):\n",
        "  x = x_0\n",
        "  H = 1.0 #extimacion inicial... recomendada por CHAT GPT :D\n",
        "\n",
        "  for i in range(max_iter):\n",
        "    grad = f_deriv(x)\n",
        "    if abs(grad) < tol:\n",
        "      break\n",
        "\n",
        "    d = -H * grad\n",
        "    x_new = x + d\n",
        "    grad_new = f_deriv(x_new)\n",
        "\n",
        "    s = x_new - x\n",
        "    y = grad_new - grad\n",
        "\n",
        "    if y!= 0: #Por si acaso\n",
        "      H = (s/y)\n",
        "    x = x_new\n",
        "\n",
        "  return x\n",
        "\n",
        "x_min = quasi_newton(f_deriv, 2.0)\n",
        "print(\"Minimum found at x =\", x_min)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = minimize(f, x0 = 2.0, jac=f_deriv, method=\"BFGS\") #It is actually BDFGS by deffect\n",
        "print(\"Minimum with BFGS:\", result.x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lg0G_20wJxK",
        "outputId": "e301233d-70cf-4a12-da90-26bce30f280e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum with BFGS: [2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Newton_Raphson Method"
      ],
      "metadata": {
        "id": "Vb_DAOe9s9ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def newton_raphson(f_deriv, f_seg_deriv, x_0, tol = 1e-9, max_iter = 1000000): #... initial point, really sensible\n",
        "  x = x_0\n",
        "  for i in range(max_iter):\n",
        "    grad = f_deriv(x)\n",
        "    hess = f_seg_deriv(x)\n",
        "    if abs(grad) < tol:\n",
        "      break\n",
        "    x -= grad/hess\n",
        "  return x\n",
        "\n",
        "x_min = newton_raphson(f_deriv, f_seg_deriv, 2.0)\n",
        "print(\"Minimum found at x =\", x_min)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3xeU4Drpgkl",
        "outputId": "2f03832c-4f65-4d2e-dae8-ad52aa12e170"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum found at x = 7.62939453125e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_new_raph = minimize(f, x0=2.0, jac=f_deriv, hess=f_seg_deriv,method=\"Newton-CG\")\n",
        "print(\"Minimum with Newton Raphson method:\", result_new_raph.x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sObVIlDoxEty",
        "outputId": "990c015e-6eee-42ee-80e4-5383a78f52a8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum with Newton Raphson method: [2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Descent Method"
      ],
      "metadata": {
        "id": "HZ0kvjvYtJJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradiente(f_deriv, x_0, ta = 0.05, tol = 1e-06, max_iter=1000): #0.05 better than 0.01\n",
        "  x = x_0\n",
        "  for i in range(max_iter):\n",
        "    grad = f_deriv(x)\n",
        "    if abs(grad) < tol:\n",
        "      break\n",
        "    x = -ta * grad\n",
        "  return x\n",
        "\n",
        "x_min = gradiente(f_deriv, 2.0)\n",
        "print(\"Minimum found at x =\", x_min)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPLQD5i9qqsJ",
        "outputId": "ac363495-3e50-4f48-99a4-67bce9fce019"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum found at x = 2.347749264605701e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.tensor([2.0], requires_grad = True) #Inicial value, 2.0\n",
        "\n",
        "optimizer = torch.optim.SGD([x], lr =0.01)\n",
        "\n",
        "for j in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  loss = x**4 - 3*x**3 + 2\n",
        "  loss.backward() #Obtain a gradient\n",
        "  optimizer.step()\n",
        "\n",
        "print(x.item()) #Minimum found\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX8xV8h2r58a",
        "outputId": "34213ce5-236c-4e56-ceeb-6fd0ba24d881"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.249999523162842\n"
          ]
        }
      ]
    }
  ]
}